from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
from contextlib import asynccontextmanager
import uvicorn
import os

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
from langchain.schema.runnable import RunnablePassthrough, RunnableConfig
from langchain.schema.output_parser import StrOutputParser

# --- Configuration ---
CHROMA_PERSIST_DIR = "./chroma_db_ollama"
OLLAMA_EMBEDDING_MODEL = "embeddinggemma:latest"
OLLAMA_LLM_MODEL = "deepseek-r1:7b"


# --- Pydantic Models  ---
class QuestionRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5
    temperature: Optional[float] = 0.3


class Document(BaseModel):
    pmid: str
    title: str
    content: str
    # score: Optional[float] = None # ChromaDB v0.4+ ä¸å†é»˜è®¤è¿”å› score


class AnswerResponse(BaseModel):
    answer: str
    references: List[Document]


class SystemStatus(BaseModel):
    status: str
    model: str
    embedding_model: str
    total_documents: int


# --- Global State ---
# ä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨å…¨å±€çŠ¶æ€ï¼Œæ›´æ¸…æ™°
app_state = {
    "vector_db": None,
    "llm": None,
    "rag_chain": None
}


# --- Lifespan Context Manager ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("\n" + "=" * 60)
    print("ğŸ¥ Medical Literature QA System Starting...")
    print("=" * 60)
    try:
        if not os.path.exists(CHROMA_PERSIST_DIR):
            raise FileNotFoundError(
                f"Vector database not found at {CHROMA_PERSIST_DIR}. Please run build_index.py first.")

        print("ğŸ”„ Initializing embeddings...")
        embedding_function = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)

        print("ğŸ”„ Loading vector database...")
        app_state["vector_db"] = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_function)

        doc_count = app_state["vector_db"]._collection.count()
        if doc_count == 0:
            print("âš ï¸ Warning: Vector database is empty.")

        print("âœ… System initialized successfully")
        print(f"   - LLM Model: {OLLAMA_LLM_MODEL}")
        print(f"   - Embedding: {OLLAMA_EMBEDDING_MODEL}")
        print(f"   - Documents: {doc_count}")
        print("=" * 60 + "\n")

    except Exception as e:
        print(f"âŒ Initialization error: {e}")

    yield

    # Shutdown
    print("\nğŸ‘‹ Shutting down...")


# --- FastAPI App ---
app = FastAPI(title="Medical Literature QA API", version="1.0.0", lifespan=lifespan)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… æ ¸å¿ƒä¿®å¤ 1: æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/static", StaticFiles(directory="static"), name="static")


# --- API Endpoints ---

# âœ… æ ¸å¿ƒä¿®å¤ 2: æ›¿æ¢æ ¹è·¯å¾„("/")çš„ç«¯ç‚¹ï¼Œè®©å®ƒè¿”å›HTMLæ–‡ä»¶
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """Serve the index.html file."""
    try:
        with open("static/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="index.html not found")


@app.get("/api/status", response_model=SystemStatus)
async def get_status():
    if app_state["vector_db"] is None:
        raise HTTPException(status_code=503, detail="System is not initialized or failed to initialize.")

    return SystemStatus(
        status="online",
        model=OLLAMA_LLM_MODEL,
        embedding_model=OLLAMA_EMBEDDING_MODEL,
        total_documents=app_state["vector_db"]._collection.count()
    )


@app.post("/api/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    if app_state["vector_db"] is None:
        raise HTTPException(status_code=503, detail="System not initialized")

    if not request.question.strip():
        raise HTTPException(status_code=400, detail="Question cannot be empty")

    try:
        print(
            f"\nğŸ“ Processing question: '{request.question[:50]}...' (Top K: {request.top_k}, Temp: {request.temperature})")

        # âœ… ä¼˜åŒ– 1: åŠ¨æ€é…ç½® retriever å’Œ llmï¼Œè€Œä¸æ˜¯é‡æ–°åˆ›å»º
        retriever = app_state["vector_db"].as_retriever(search_kwargs={"k": request.top_k})
        llm = Ollama(model=OLLAMA_LLM_MODEL, temperature=request.temperature)

        template = """You are a professional medical research assistant. Use the following context to answer the question. 
        If you don't know, just say that you don't know. Answer in Chinese.
        Context: {context}
        Question: {question}
        Answer (in Chinese):"""
        prompt = ChatPromptTemplate.from_template(template)

        chain = (
                {"context": retriever, "question": RunnablePassthrough()}
                | prompt
                | llm
                | StrOutputParser()
        )

        # âœ… ä¼˜åŒ– 2: ä½¿ç”¨ LangChain çš„åŸç”Ÿå¼‚æ­¥æ–¹æ³• .ainvoke()
        answer = await chain.ainvoke(request.question)

        # å¼‚æ­¥è·å–å‚è€ƒæ–‡çŒ®
        docs = await retriever.ainvoke(request.question)

        references = [
            Document(
                pmid=doc.metadata.get('pmid', 'N/A'),
                title=doc.metadata.get('title', 'No Title'),
                content=doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
            ) for doc in docs
        ]

        print(f"âœ… Answer generated with {len(references)} references.")
        return AnswerResponse(answer=answer, references=references)

    except Exception as e:
        print(f"âŒ Error processing question: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --- Run Server ---
if __name__ == "__main__":
    # ç§»é™¤ reload=True, uvicorn.run()æœ¬èº«ä¸æ”¯æŒï¼Œåº”åœ¨å‘½ä»¤è¡Œä½¿ç”¨
    uvicorn.run("backend:app", host="0.0.0.0", port=8000, reload=True)